---
title: "R Notebook"
output: html_notebook
---
 

```{r}

# +++++++++++++++++ #
#                   # 
#    LIBRARIES
#                   #
# +++++++++++++++++ #

# Installation of required packages

install.packages("tableone")
install.packages("Matching")
install.packages("MatchIt")

install.packages("ipw")
install.packages("survey")

install.packages("data.table")


# Load libraries


library(tableone)
library(Matching)
library(MatchIt)

library(ipw)
library(survey)

library(utils)
library(dplyr)
library(data.table) 


#Load data

data(lalonde)

 
dat <- lalonde
View(dat)


```

```{r}
# +++++++++++++++++ #
#                   # 
# QUESTION 1: 
# Fit a propensity score model. Use a logistic regression model, 
# where the outcome is treatment. Include the 8 confounding variables in
# the model as predictors, with no interaction terms or non-linear terms (such as
# squared terms). Obtain the propensity score for each subject. Next, obtain the
# inverse probability of treatment weights for each subject.  
#
# What are the minimum and maximum weights?    
#                   #
# +++++++++++++++++ #

# list covariates in one group
xvars <- c("age","educ","race","married","nodegree","re74","re75")

# fit a propensity model (likelihood of treatment = outcome of interest)
pmodel <- glm(treat ~ age + educ + race + married + nodegree + re74 + re75,
              family = binomial(),
              data =dat)

# pull out propensity scores
#pscores <- pmodel$fitted.values
dat$ps <- predict(pmodel, type = "response")

# create weights
dat$weight <- ifelse(dat$treat==1,1/(dat$ps),1/(1-dat$ps))

# get min and max

summary(dat$weight)

# Min 1.01
# Max 40.1

```
 


```{r}
# +++++++++++++++++ #
#                   # 
# QUESTION 2: 
# Find the standardized differences for each
# confounder on the weighted (pseudo) population. What is the standardized
# difference for nodegree?    
#                   #
# +++++++++++++++++ #

# Use weights to create pseudo population
weighted.dat <- svydesign(ids = ~1, data = dat, weights = ~weight)

# use package to create a 'table one' 
weighted.table <- svyCreateTableOne(vars= xvars, 
                                    strata = "treat",
                                    data = weighted.dat)

# show smds
print(weighted.table, smd = TRUE)

# standardized diff for nodegree is 0.112

```

```{r}

# +++++++++++++++++ #
#                   # 
# QUESTION 3: 
# Using IPTW, find the estimate and 95% confidence
# interval for the average causal effect. This can be obtained from svyglm
#
# +++++++++++++++++ #

# fit a marginal structural model
msm <- svyglm(re78 ~ treat, design = weighted.dat)

# get difference, and CI
coef(msm) # treatment 224.68
confint(msm) # IC [-1562.9 , 2012.2]

```

```{r}
# +++++++++++++++++ #
#                   # 
# QUESTION 4: 
# Now truncate the weights at the 1st and 99th percentiles. 
# Using IPTW with the truncated weights, find the
# estimate and 95% confidence interval for the average causal effect 
#
# +++++++++++++++++ #

# redo the underlying weighted model using ipw library and use the built-in trunc setting
weighted.model <- ipwpoint(exposure = treat,
                           family = "binomial",
                           link= "logit",
                           denominator = ~age + educ + race + married + nodegree + re74 + re75,
                           data = dat,
                           trunc = 0.01)
  

# use survey package again, but with new weights
dat$wt <- weighted.model$weights.trunc

msm.truncated <- svyglm(re78 ~ treat, design = svydesign(~1,
                                                         weights= ~wt,
                                                         data = dat))
# get values of interest:
coef(msm.truncated) # treat 486.94
confint(msm.truncated) # CI -1093.8, 2067.7

```
